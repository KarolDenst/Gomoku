{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Kwd8Gfnhj87B"},"outputs":[],"source":["import torch\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from tqdm.notebook import trange\n","torch.manual_seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20008,"status":"ok","timestamp":1716840587885,"user":{"displayName":"Karol Denst","userId":"00469011533726302230"},"user_tz":-120},"id":"K6CVkWhjks5s","outputId":"1b0c2df3-9fc7-4a0b-ed22-29263d2dfea3"},"outputs":[],"source":["def in_colab():\n","    try:\n","        import google.colab\n","        return True\n","    except ImportError:\n","        return False\n","\n","if in_colab():\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    base_path = '/content/drive/My Drive/Colab Notebooks'\n","else:\n","    base_path = '.'\n","\n","model_path = os.path.join(base_path, 'alpha_gomoku/model')\n","optimizer_path = os.path.join(base_path, 'alpha_gomoku/optimizer')\n","\n","os.makedirs(model_path, exist_ok=True)\n","os.makedirs(optimizer_path, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EMPTY = 0\n","PLAYER1 = 1\n","PLAYER2 = -1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Gomoku:\n","    \"\"\"\n","    Static class that contains methods related to Gomoku.\n","    \"\"\"\n","    \n","    def __init__(self, size=15):\n","        self.size = size\n","\n","    def get_initial_state(self):\n","        \"\"\"\n","        Create an empty board.\n","\n","        Returns:\n","            np.array: A 2D array representing the board.\n","        \"\"\"\n","        return np.zeros((self.size, self.size)).astype(np.int8)\n","\n","    def get_next_state(self, state, action, player):\n","        \"\"\"\n","        Creates a new state by applying the action to the current state.\n","\n","        Args:\n","            state (np.array): The current state of the game.\n","            action (int): The action to apply to the state. Represented as an integer from 0 to size^2 - 1.\n","            player (int): The player to apply the action. Should be either PLAYER1 or PLAYER2.\n","\n","        Raises:\n","            ValueError: The action is invalid. The cell is already occupied.\n","\n","        Returns:\n","            np.array: A 2D array representing the new state of the game.\n","        \"\"\"\n","        row = action // self.size\n","        col = action % self.size\n","        if state[row, col] != EMPTY:\n","            raise ValueError(\"Invalid action\")\n","        state[row, col] = player\n","        return state\n","\n","    def get_moves(self, state):\n","        \"\"\"\n","        Get all the legal actions for the current state.\n","\n","        Args:\n","            state (np.array): The current state of the game.\n","\n","        Returns:\n","            np.array: A 1D array of size size^2 representing the legal actions.\n","        \"\"\"\n","        return (state.reshape(-1) == EMPTY).astype(np.uint8)\n","\n","    def check_win(self, state, action):\n","        \"\"\"\n","        Checks if the action won the game.\n","\n","        Args:\n","            state (np.array): The current state of the game.\n","            action (int): Last action made.\n","\n","        Returns:\n","            boolean: True if the action won the game, False otherwise.\n","        \"\"\"\n","        if action is None:\n","            return False\n","\n","        row = action // self.size\n","        col = action % self.size\n","        player = state[row, col]\n","        if player == EMPTY:\n","            return False\n","\n","        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n","\n","        for dr, dc in directions:\n","            count = 1\n","\n","            # Create an array of indices for positive direction\n","            indices = np.array([(row + i * dr, col + i * dc) for i in range(1, 5)])\n","            valid_indices = (\n","                (indices[:, 0] >= 0)\n","                & (indices[:, 0] < self.size)\n","                & (indices[:, 1] >= 0)\n","                & (indices[:, 1] < self.size)\n","            )\n","            valid_indices = indices[valid_indices]\n","\n","            count += np.sum(state[valid_indices[:, 0], valid_indices[:, 1]] == player)\n","\n","            # Create an array of indices for negative direction\n","            indices = np.array([(row - i * dr, col - i * dc) for i in range(1, 5)])\n","            valid_indices = (\n","                (indices[:, 0] >= 0)\n","                & (indices[:, 0] < self.size)\n","                & (indices[:, 1] >= 0)\n","                & (indices[:, 1] < self.size)\n","            )\n","            valid_indices = indices[valid_indices]\n","\n","            count += np.sum(state[valid_indices[:, 0], valid_indices[:, 1]] == player)\n","\n","            if count >= 5:\n","                return True\n","\n","        return False\n","\n","    def get_value_and_terminated(self, state, action):\n","        \"\"\"Returns the value of the state and whether the game is terminated.\n","\n","        Args:\n","            state (np.array): The current state of the game.\n","            action (int): The last action made.\n","\n","        Returns:\n","            (int, boolean): The value of the state and whether the game is terminated.\n","        \"\"\"\n","        if self.check_win(state, action):\n","            return 1, True\n","        if np.sum(state == EMPTY) == 0:\n","            return 0, True\n","        return 0, False\n","\n","    def get_opponent(self, player):\n","        \"\"\"\n","        Returns the opponent of the given player.\n","\n","        Args:\n","            player (int): The current player.\n","\n","        Returns:\n","            int: The opponent.\n","        \"\"\"\n","        return PLAYER1 if player == PLAYER2 else PLAYER2\n","\n","    def get_opponent_value(self, player):\n","        \"\"\"\n","        _summary_\n","\n","        Args:\n","            player (_type_): _description_\n","\n","        Returns:\n","            _type_: _description_\n","        \"\"\"\n","        return -player\n","\n","    def change_perspective(self, state, player):\n","        \"\"\"\n","        Returns the state from the perspective of the given player. Also works for batched states.\n","\n","        Args:\n","            state (np.array): The current state of the game.\n","            player (int): The player to change the perspective to.\n","\n","        Returns:\n","            np.array: The new state of the game.\n","        \"\"\"\n","        return state * player\n","\n","    def get_encoded_state(self, state):\n","        \"\"\"\n","        Encodes the game state into a multi-channel array.\n","\n","        Args:\n","            state (np.array): The game state represented as a 2D or 3D numpy array.\n","\n","        Returns:\n","            np.array: The encoded game state as a multi-channel array.\n","        \"\"\"\n","        encoded_state = np.stack(\n","            (state == PLAYER1, state == PLAYER2, state == EMPTY)\n","        ).astype(np.float32)\n","\n","        if len(state.shape) == 3:\n","            encoded_state = np.swapaxes(encoded_state, 0, 1)\n","\n","        return encoded_state\n","\n","    def print(self, state):\n","        \"\"\"\n","        Prints the current state of the Gomoku board.\n","\n","        Args:\n","            state (np.array): The current state of the Gomoku board.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        board_str = \"\"\n","        for row in range(self.size):\n","            row_str = \" \".join(\n","                str(state[row, col]) if state[row, col] != EMPTY else \".\"\n","                for col in range(self.size)\n","            )\n","            board_str += row_str + \"\\n\"\n","        board_str = board_str.replace(\"-1\", \"O\").replace(\"1\", \"X\")\n","        print(board_str)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Node:\n","    def __init__(self, game, args, state, parent=None, action=None, prior=0):\n","        self.game = game\n","        self.args = args\n","        self.state = state\n","        self.parent = parent\n","        self.action = action\n","        self.prior = prior\n","\n","        self.children = []\n","        self.visit_count = 0\n","        self.total_value = 0\n","\n","    def is_fully_expanded(self):\n","        \"\"\"\n","        Checks if the node is fully expanded.\n","\n","        Returns:\n","            bool: True if the node is fully expanded, False otherwise.\n","        \"\"\"\n","        return len(self.children) > 0\n","\n","    def select(self):\n","        \"\"\"\n","        Selects the best child node based on the Upper Confidence Bound (UCB) value.\n","\n","        Returns:\n","            The best child node.\n","        \"\"\"\n","        best_child = None\n","        best_ucb = -np.inf\n","        for child in self.children:\n","            ucb = self.get_ucb(child)\n","            if ucb > best_ucb:\n","                best_ucb = ucb\n","                best_child = child\n","\n","        return best_child\n","\n","    def get_ucb(self, child):\n","        \"\"\"\n","        Calculates the Upper Confidence Bound (UCB) value for a given child node.\n","\n","        Parameters:\n","            child (Node): The child node for which to calculate the UCB value.\n","\n","        Returns:\n","            float: The UCB value for the child node.\n","        \"\"\"\n","        if child.visit_count == 0:\n","            q_value = 0\n","        else:\n","            q_value = 1 - (child.total_value / child.visit_count + 1) / 2\n","        return (\n","            q_value\n","            + self.args[\"C\"]\n","            * np.sqrt(self.visit_count)\n","            / (child.visit_count + 1)\n","            * child.prior\n","        )\n","\n","    def expand(self, policy):\n","        \"\"\"\n","        Expands the current node by creating child nodes for each possible action.\n","\n","        Args:\n","            policy (list): A list of probabilities for each possible action.\n","\n","        Returns:\n","            child (Node): The child node that was created.\n","        \"\"\"\n","        for action, prob in enumerate(policy):\n","            if prob > 0:\n","                child_state = self.state.copy()\n","                child_state = self.game.get_next_state(child_state, action, PLAYER1)\n","                child_state = self.game.change_perspective(child_state, player=PLAYER2)\n","\n","                child = Node(self.game, self.args, child_state, self, action, prob)\n","                self.children.append(child)\n","\n","        return child\n","\n","    def backpropagate(self, value):\n","        \"\"\"\n","        Backpropagate the value obtained from the simulation to update the statistics of the current node and its ancestors.\n","\n","        Args:\n","            value: The value obtained from the simulation.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        self.visit_count += 1\n","        self.total_value += value\n","        if self.parent is not None:\n","            value = self.game.get_opponent_value(value)\n","            self.parent.backpropagate(value)\n","\n","\n","class AlphaMCTS:\n","    def __init__(self, game, args, model):\n","        self.game = game\n","        self.args = args\n","        self.model = model\n","\n","    @torch.no_grad()\n","    def search(self, state):\n","        \"\"\"\n","        Performs Monte Carlo Tree Search (MCTS) to find the best action to take.\n","\n","        Args:\n","            state (np.array): The current state of the game.\n","\n","        Returns:\n","            np.array: An array of action probabilities, indicating the likelihood of selecting each action.\n","        \"\"\"\n","        root = Node(self.game, self.args, state)\n","        for _ in range(self.args[\"num_searches\"]):\n","            node = root\n","\n","            # selection\n","            while node.is_fully_expanded():\n","                node = node.select()\n","\n","            value, terminated = self.game.get_value_and_terminated(\n","                node.state, node.action\n","            )\n","            value = self.game.get_opponent_value(value)\n","\n","            if not terminated:\n","                policy, value = self.model(\n","                    torch.tensor(\n","                        self.game.get_encoded_state(node.state),\n","                        device=self.model.device,\n","                    ).unsqueeze(0)\n","                )\n","                policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n","                valid_moves = self.game.get_moves(node.state)\n","                policy = policy * valid_moves\n","                policy /= np.sum(policy)\n","\n","                value = value.item()\n","                node.expand(policy)\n","\n","            node.backpropagate(value)\n","\n","        action_probs = np.zeros(self.game.size * self.game.size)\n","        for child in root.children:\n","            action_probs[child.action] = child.visit_count\n","        action_probs = action_probs / np.sum(action_probs)\n","\n","        return action_probs\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ResBlock(nn.Module):\n","    def __init__(self, num_hidden):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(num_hidden)\n","        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(num_hidden)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = F.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = out + x\n","        out = F.relu(out)\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, size, num_blocks, num_hidden, device):\n","        super().__init__()\n","\n","        self.device = device\n","        self.startBlock = nn.Sequential(\n","            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(num_hidden),\n","            nn.ReLU()\n","        )\n","\n","        self.backBone = nn.ModuleList(\n","            [ResBlock(num_hidden) for _ in range(num_blocks)]\n","        )\n","\n","        self.policyHead = nn.Sequential(\n","            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(size * size * 32, size * size)\n","        )\n","\n","        self.valueHead = nn.Sequential(\n","            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(3),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3 * size * size, 1),\n","            nn.Tanh()\n","        )\n","\n","        self.to(device)\n","\n","    def forward(self, x):\n","        x = self.startBlock(x)\n","        for block in self.backBone:\n","            x = block(x)\n","        policy = self.policyHead(x)\n","        value = self.valueHead(x)\n","\n","        return policy, value"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TrainingMCTS:\n","    def __init__(self, game, args, model):\n","        self.game = game\n","        self.args = args\n","        self.model = model\n","\n","    @torch.no_grad()\n","    def search(self, states, self_play_games):\n","        policy, _ = self.model(\n","            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n","        )\n","        policy = torch.softmax(policy, axis=1).cpu().numpy()\n","\n","        for i, game in enumerate(self_play_games):\n","            game_policy = policy[i]\n","            valid_moves = self.game.get_moves(states[i])\n","            game_policy *= valid_moves\n","            game_policy /= np.sum(game_policy)\n","\n","            game.root = Node(self.game, self.args, states[i])\n","            game.root.expand(game_policy)\n","\n","        for _ in range(self.args[\"num_searches\"]):\n","            for game in self_play_games:\n","                game.node = None\n","                node = game.root\n","\n","                while node.is_fully_expanded():\n","                    node = node.select()\n","\n","                value, terminated = self.game.get_value_and_terminated(\n","                    node.state, node.action\n","                )\n","                value = self.game.get_opponent_value(value)\n","\n","                if terminated:\n","                    node.backpropagate(value)\n","                else:\n","                    game.node = node\n","\n","            expandable_games = [\n","                idx\n","                for idx in range(len(self_play_games))\n","                if self_play_games[idx].node is not None\n","            ]\n","            if len(expandable_games) > 0:\n","                states = np.stack(\n","                    [self_play_games[idx].node.state for idx in expandable_games]\n","                )\n","                policy, value = self.model(\n","                    torch.tensor(\n","                        self.game.get_encoded_state(states), device=self.model.device\n","                    )\n","                )\n","                policy = torch.softmax(policy, axis=1).detach().cpu().numpy()\n","                value = value.cpu().numpy()\n","\n","            for i, idx in enumerate(expandable_games):\n","                node = self_play_games[idx].node\n","                self_play_policy, self_play_value = policy[i], value[i]\n","\n","                valid_moves = self.game.get_moves(node.state)\n","                self_play_policy = self_play_policy * valid_moves\n","                self_play_policy /= np.sum(self_play_policy)\n","                node.expand(self_play_policy)\n","                node.backpropagate(self_play_value)\n","\n","\n","class SelfPlayGame:\n","    def __init__(self, game):\n","        self.state = game.get_initial_state()\n","        self.memory = []\n","        self.root = None\n","        self.node = None\n","\n","\n","class AlphaZero:\n","    def __init__(self, model, optimizer, game, args):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.game = game\n","        self.args = args\n","        self.mcts = TrainingMCTS(game, args, model)\n","\n","    def self_play(self):\n","        return_memory = []\n","        player = PLAYER1\n","        self_paly_games = [\n","            SelfPlayGame(self.game) for _ in range(self.args[\"num_parallel_games\"])\n","        ]\n","\n","        while len(self_paly_games) > 0:\n","            states = np.stack([game.state for game in self_paly_games])\n","            neutral_states = self.game.change_perspective(states, player)\n","            self.mcts.search(neutral_states, self_paly_games)\n","\n","            for i in range(len(self_paly_games))[::-1]:\n","                game = self_paly_games[i]\n","\n","                action_probs = np.zeros(self.game.size * self.game.size)\n","                for child in game.root.children:\n","                    action_probs[child.action] = child.visit_count\n","                action_probs = action_probs / np.sum(action_probs)\n","\n","                game.memory.append((game.root.state, action_probs, player))\n","                action = np.random.choice(\n","                    self.game.size * self.game.size, p=action_probs\n","                )\n","                game.state = self.game.get_next_state(game.state, action, player)\n","                value, terminated = self.game.get_value_and_terminated(\n","                    game.state, action\n","                )\n","                if terminated:\n","                    for state, action_probs, player in game.memory:\n","                        outcome = (\n","                            value\n","                            if player == PLAYER1\n","                            else self.game.get_opponent_value(value)\n","                        )\n","                        return_memory.append(\n","                            (self.game.get_encoded_state(state), action_probs, outcome)\n","                        )\n","                    del self_paly_games[i]\n","\n","            player = self.game.get_opponent(player)\n","\n","        return return_memory\n","\n","    def train(self, memory):\n","        random.shuffle(memory)\n","        for batch_index in range(0, len(memory), self.args[\"batch_size\"]):\n","            batch = memory[\n","                batch_index : min(len(memory) - 1, batch_index)\n","                + self.args[\"batch_size\"]\n","            ]\n","            states, policy_targets, value_targets = zip(*batch)\n","\n","            states, policy_targets, value_targets = (\n","                np.array(states),\n","                np.array(policy_targets),\n","                np.array(value_targets).reshape(-1, 1),\n","            )\n","            states = torch.tensor(states, dtype=torch.float32, device=self.model.device)\n","            policy_targets = torch.tensor(\n","                policy_targets, dtype=torch.float32, device=self.model.device\n","            )\n","            value_targets = torch.tensor(\n","                value_targets, dtype=torch.float32, device=self.model.device\n","            )\n","\n","            out_policy, out_value = self.model(states)\n","\n","            policy_loss = F.cross_entropy(out_policy, policy_targets)\n","            value_loss = F.mse_loss(out_value, value_targets)\n","            loss = policy_loss + value_loss\n","\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","\n","    def learn(self):\n","        for iteration in trange(self.args[\"num_iterations\"]):\n","            memory = []\n","\n","            self.model.eval()\n","            for play_iteration in trange(\n","                self.args[\"num_self_play_iterations\"] // self.args[\"num_parallel_games\"]\n","            ):\n","                memory += self.self_play()\n","\n","            self.model.train()\n","            for epoch in range(self.args[\"num_epochs\"]):\n","                self.train(memory)\n","\n","            torch.save(\n","                self.model.state_dict(),\n","                f'{self.args[\"model_path\"]}/model_{iteration}.pt',\n","            )\n","            torch.save(\n","                self.optimizer.state_dict(),\n","                f'{self.args[\"optimizer_path\"]}/optimizer_{iteration}.pt',\n","            )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["493696a08b5d40b3b12d1f6eaaffe458","bc5401e642aa4457beb750f3712b7cba","6b4ea5e95ef448118ea401bfc90cd4a4","a648b52e5b3b4244a6266e44c71d2e2b","f7cd3d1660b04a43a2091b3eeeb6f0b3","d237ee9cbd1442b2a27afab6500b9ab3","7f5bb95122be4f1eb37d1ad4efbd7591","5dd0f606a3f64abd80c415a641034667","c6a5d5f0607a4c8ba6e60025b0ba5082","ac5e6c49fbbd4a0fa1eaf72dbb363b1d","d34e5f9274644418b21f99d067e09251","998acd75d3644ad5ae75376ed2c44f33","1137c56f245b4ab5858c91e5d919c608","9a1c962d9edb4c848474ebb627342911","cddd0cbb0d8a416b876014f95e028c6b","5e24a48884ef46138cfb01e699a70d81","136680060d3943048b8b285b9576bf13","e8adff4236b7420d8e2347311cad0799","266422b38c92412387d8c5a5645748c2","d61c4636053b480b82baac1182d500cf","9f4f950d0fe74e1a9d6e3b1c32757bf1","973db2695b01442aaa79ebaba19fed1a","157335ebc84642c682c94e8d07c53136","0d7fdce8baf14f5b94fa0a836f438537"]},"executionInfo":{"elapsed":1244854,"status":"ok","timestamp":1716846904413,"user":{"displayName":"Karol Denst","userId":"00469011533726302230"},"user_tz":-120},"id":"oksHef6uj87E","outputId":"7aa4c1c4-9280-4494-b58d-b8f6d405ac69"},"outputs":[],"source":["BOARD_SIZE = 10\n","\n","game = Gomoku(BOARD_SIZE)\n","num_blocks = 16\n","num_hidden = 256\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = ResNet(game.size, num_blocks, num_hidden, device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","args = {\n","    'C': 2,\n","    'num_searches': 500,\n","    'num_iterations': 3,\n","    'num_self_play_iterations': 200,\n","    'num_epochs': 4,\n","    'num_parallel_games': 200,\n","    'batch_size': 128,\n","\n","    'model_path': model_path,\n","    'optimizer_path': optimizer_path\n","}\n","\n","alphaZero = AlphaZero(model, optimizer, game, args)\n","alphaZero.learn()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNwmmpxrj87D"},"outputs":[],"source":["game = Gomoku(BOARD_SIZE)\n","\n","state = game.get_initial_state()\n","actions = [0, 7, 1, 8, 2, 9, 3, 10]\n","player = PLAYER1\n","for action in actions:\n","    state = game.get_next_state(state, action, player)\n","    player = game.get_opponent(player)\n","\n","game.print(state)\n","\n","tensor_state = torch.tensor(game.get_encoded_state(state)).unsqueeze(0)\n","\n","model = ResNet(game.size, num_blocks, num_hidden, device)\n","model.load_state_dict(torch.load(f'{model_path}/model_{args[\"num_iterations\"] - 1}.pt'))\n","model.eval()\n","\n","policy, value = model(tensor_state)\n","value = value.item()\n","policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n","\n","print(value, policy)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1137c56f245b4ab5858c91e5d919c608":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_136680060d3943048b8b285b9576bf13","placeholder":"​","style":"IPY_MODEL_e8adff4236b7420d8e2347311cad0799","value":"  0%"}},"136680060d3943048b8b285b9576bf13":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"266422b38c92412387d8c5a5645748c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"493696a08b5d40b3b12d1f6eaaffe458":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc5401e642aa4457beb750f3712b7cba","IPY_MODEL_6b4ea5e95ef448118ea401bfc90cd4a4","IPY_MODEL_a648b52e5b3b4244a6266e44c71d2e2b"],"layout":"IPY_MODEL_f7cd3d1660b04a43a2091b3eeeb6f0b3"}},"5dd0f606a3f64abd80c415a641034667":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e24a48884ef46138cfb01e699a70d81":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b4ea5e95ef448118ea401bfc90cd4a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dd0f606a3f64abd80c415a641034667","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6a5d5f0607a4c8ba6e60025b0ba5082","value":3}},"7f5bb95122be4f1eb37d1ad4efbd7591":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"973db2695b01442aaa79ebaba19fed1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"998acd75d3644ad5ae75376ed2c44f33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1137c56f245b4ab5858c91e5d919c608","IPY_MODEL_9a1c962d9edb4c848474ebb627342911","IPY_MODEL_cddd0cbb0d8a416b876014f95e028c6b"],"layout":"IPY_MODEL_5e24a48884ef46138cfb01e699a70d81"}},"9a1c962d9edb4c848474ebb627342911":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_266422b38c92412387d8c5a5645748c2","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d61c4636053b480b82baac1182d500cf","value":0}},"9f4f950d0fe74e1a9d6e3b1c32757bf1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a648b52e5b3b4244a6266e44c71d2e2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac5e6c49fbbd4a0fa1eaf72dbb363b1d","placeholder":"​","style":"IPY_MODEL_d34e5f9274644418b21f99d067e09251","value":" 3/3 [1:43:27&lt;00:00, 2043.28s/it]"}},"ac5e6c49fbbd4a0fa1eaf72dbb363b1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc5401e642aa4457beb750f3712b7cba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d237ee9cbd1442b2a27afab6500b9ab3","placeholder":"​","style":"IPY_MODEL_7f5bb95122be4f1eb37d1ad4efbd7591","value":"100%"}},"c6a5d5f0607a4c8ba6e60025b0ba5082":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cddd0cbb0d8a416b876014f95e028c6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f4f950d0fe74e1a9d6e3b1c32757bf1","placeholder":"​","style":"IPY_MODEL_973db2695b01442aaa79ebaba19fed1a","value":" 0/50 [00:00&lt;?, ?it/s]"}},"d237ee9cbd1442b2a27afab6500b9ab3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d34e5f9274644418b21f99d067e09251":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d61c4636053b480b82baac1182d500cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8adff4236b7420d8e2347311cad0799":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7cd3d1660b04a43a2091b3eeeb6f0b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
